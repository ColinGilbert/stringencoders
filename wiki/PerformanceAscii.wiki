#summary Performance for ascii encoders
#labels Featured


== Performance Numbers ==

AMD / Linux / gLibc

{{{
$ date
Mon Jun  4 21:04:58 UTC 2007
$ ./speedtest_ascii
type    clib    direct  map     para    hsieh1  hsieh2  Final   improvement
toupper 590000  590000  580000  260000  190000  190000  230000  2.6x
}}}

Intel Core Duo / Mac OS X

{{{
$ date
Mon Jun  4 17:03:56 EDT 2007
$ ./speedtest_ascii
type    clib    direct  map     para    hsieh1  hsieh2  Final   improvement
toupper 467     81      85      66      33      32      33      14.2x
}}}

G4

Something is very wrong with the optimizer/compiler/os if we can get a 62.5x performance improvement.

{{{
$ date
Tue Jun  5 00:05:26 EDT 2007
$ ./speedtest_ascii
type    clib    direct  map     para    hsieh1  hsieh2  Final   improvement
toupper 3249    136     110     71      40      48      52      62.5x
}}}


== ToUpper Algorithms ==

=== Sequential Ctypes toupper ===
A simple example is doing upper casing an ascii string.  A standard implementation might look like

{{{
void toupper_copy1(char* dest, const char* str, int len)
{
    int i;
    for (i = 0; i < len; ++i) {
        // toupper is defined in <ctype.h>                                                                                              
        *dest++ = toupper(str[i]);
    }
    *dest = 0;
}
}}}

The problem here is some clibs have crap versions of `toupper`.  They have to look up the 'C Locale' and use a complicated table and bit shifts.  A better version might be


=== Sequential Direct Computation ===

{{{
void toupper_copy2(char* dest, const char* str, int len)
{
    int i;
    unsigned char c;
    for (i = 0; i < len; ++i) {
        c = str[i];
        *dest++ = (c >= 'a' && c <= 'z') ? c : (c -32);
    }
    *dest = 0;
}
}}}

One some platforms, this simple version is quite a bit faster already. 


=== Sequential Table Lookup ===

The problem here is that for each byte we have to do two comparisons and maybe a subtraction.  This can all be precomputed.

{{{
static char toupper_map = {0x00, 0x01, ... 'A' .. 'Z' ... 'A', .. 'Z'... 0xFF};

void toupper_copy3(char* dest, const char* str, int len)
{
    int i;
    for (i = 0; i < len; ++i) {
        *dest++ = toupper_map[(unsigned char)(str[i])];
    }
    *dest = 0;
}
}}}

This may or may not be faster still depending on your compiler or CPU.  


=== Parallel Table Lookup ===
However modern CPUs can have multiple registers and can either do multiple things at once or pipeline multiple request.  For instance

{{{
void toupper_copy4(char* dest, const char* str, int len)
    const int leftover = len % 4;
    const int imax = len - leftover;
    const uint8_t* s = (const uint8_t*) str;
    for (i = 0; i != imax ; i+=4) {
        /*                                                                                                                              
          it's important to make these variables                                                                                        
          it helps the optimizer to figure out what to do                                                                               
        */
        c1 = s[i], c2=s[i+1], c3=s[i+2], c4=s[i+3];
        dest[0] = gsToUpperMap[c1];
        dest[1] = gsToUpperMap[c2];
        dest[2] = gsToUpperMap[c3];
        dest[3] = gsToUpperMap[c4];
        dest += 4;
    }
    switch (leftover) {
    case 3: *dest++ = gsToUpperMap[s[i++]];
    case 2: *dest++ = gsToUpperMap[s[i++]];
    case 1: *dest++ = gsToUpperMap[s[i]];
    case 0: *dest = '\0';
    }
}
}}}

This does things in chunks of 4.  On AMD processors this is twice as fast, on Intel, _nine_ times faster, and on my old Powerbook G4, _44 times faster_.  For details see PerformanceAscii.


=== The Hsieh Algorithm, Take 1 ===

The above trick works quite well, but all those char -> unsigned char -> int conversions and table looks takes time.  Even better is treat the string as if it were a array of integers, not chars.

{{{
 * Based code from Paul Hsieh
 * http://www.azillionmonkeys.com/qed/asmexample.html
 *
 * This was his "improved" version, but it appears to either run just
 * as fast, or a bit slower than his original version
 */
void toupper_copy5(char* dest, const char* str, int len)
{
    int i;
    uint32_t eax,ebx,ecx,edx;
    const uint8_t* ustr = (const uint8_t*) str;
    const int leftover = len % 4;
    const int imax = len / 4;
    const uint32_t* s = (const uint32_t*) str;
    uint32_t* d = (uint32_t*) dest;
    for (i = 0; i != imax; ++i) {
        eax = s[i];
        ebx = 0x80808080ul | eax;
        ecx = ebx - 0x61616161ul;
        edx = ~(ebx - 0x7b7b7b7bul);
        ebx = (ecx & edx) & (~eax & 0x80808080ul);
        *d++ = eax - (ebx >> 2);
    }

    i = imax*4;
    dest = (char*) d;
    switch (leftover) {
        case 3: *dest++ = gsToUpperMap[ustr[i++]];
        case 2: *dest++ = gsToUpperMap[ustr[i++]];
        case 1: *dest++ = gsToUpperMap[ustr[i]];
        case 0: *dest = '\0';
    }
}
}}}


On some platforms this is the fastest I've seen (but at this stage we are splitting hairs).

=== The Hsieh Algorithm, Take 2 ===

Hsieh provided an earlier version of the toupper algorithm, but it's only as raw ASM source.  The most direct translation is 

{{{
for (i = 0; i != imax; ++i) {
    eax = s[i];
    ebx = 0x7f7f7f7f;
    edx = 0x7f7f7f7f;
    ebx = ebx & eax;
    ebx = ebx + 0x05050505;
    ecx = eax;
    ecx = ~ecx;
    ebx = ebx & edx;
    ebx = ebx + 0x1a1a1a1a;
    ebx = ebx & ecx;
    ebx = ebx >> 2;
    ebx = ebx & 0x20202020;
    eax = eax - ebx;
    *d++ = eax;
}
}}}

=== The Hsieh Algorithm, Take 3, Final ===

The "De-Asmed" version of the above algorithm is something like this.  On some platforms this is a bit slower than the previous.

{{{
for (i = 0; i != imax; ++i) {
    eax = s[i];
    ebx = (0x7f7f7f7ful & eax) + 0x05050505ul;
    ebx = (0x7f7f7f7ful & ebx) + 0x1a1a1a1aul;
    ebx = ((ebx & ~eax) >> 2 ) & 0x20202020ul;
    *d++ = eax - ebx;
}
}}}
